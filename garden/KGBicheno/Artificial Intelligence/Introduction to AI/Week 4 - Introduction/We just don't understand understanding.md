# We just don't understand understanding

One of the most fundamental problems we have in building a strong AI is that we don't truly understand how we understand the world, and whether or not that's the best way to approach creating a machine that can understand the world. 

First we need to build more sophisticated ways of extracting information from the world around us. Then we need to build more sophisticated ways of linking that information together (reasoning). We also need to build more sophisticated ways of abstracting that information without human intervention. 

See [IBM talks to Yoshua Bengio](https://www.ibm.com/watson/advantage-reports/future-of-artificial-intelligence/yoshua-bengio.html)
See [IBM talks to Kevin Kelly](https://www.ibm.com/watson/advantage-reports/future-of-artificial-intelligence/kevin-kelly.html)

	In retrospect, the idea that one could program a system as intricate and involved as human vision over a summer is a ludicrous proposition, but the point was that at the time, theoretical psychologists and others had little understanding of just how much was involved.
	
See [IBM talks to Margaret Boden](https://www.ibm.com/watson/advantage-reports/future-of-artificial-intelligence/margaret-boden.html)

#ToDo fill this page out with previous mentions of:
- understanding
- cognition
- strong AI
- abstraction
- learning
- constraints