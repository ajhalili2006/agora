# long short-term memory - LSTM

Go to [[Week 2 - Introduction]] or back to the [[Main AI Page]]
Part of the pages on [[Artificial Intelligence/Introduction to AI/Week 2 - Introduction/Natural Language Processing]] and [[Attention Mechanism]].

A deep learning architecture in the same way [[CNNS - Convolutional neural networks]] are.

These algorithms are able to forget, according to the RNN section of [the beginners' guide to NLP](https://developer.ibm.com/articles/a-beginners-guide-to-natural-language-processing/) 

According to wikipedia:

	A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. 

Source - [Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory).

An LSTM is made up of a:

- cell
- an input gate
- an output gate
- a forget gate

[![A video on how LSTMs work](https://i.imgur.com/sPh0FTo.png)](https://www.youtube.com/watch?v=8HyCNIVRbSU "How LSTMs work")

The difference between LSTMs and [[Gated Recurrent Unit|GRUs]]

![An LSTM and a GRU side-by-side](https://i.imgur.com/xsBvobq.png)